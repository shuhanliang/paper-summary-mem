name: Update Academic Data

on:
  schedule:
    # 每个工作日（周一至周五）北京时间早上8:00运行（UTC时间0:00）
    - cron: '0 0 * * 1-5'
  # 允许手动触发
  workflow_dispatch:

jobs:
  update-data:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'

      # 增加模型缓存步骤
      - name: Cache Hugging Face models
        uses: actions/cache@v4
        with:
          path: ./.cache/huggingface/hub  # 将将缓存路径指定为项目工作区内的一个明确目录
          key: ${{ runner.os }}-transformers-cache-v3 #  更新key的版本号以确保使用新路径

      - name: Install dependencies
        run: npm install node-fetch@2 dotenv @xenova/transformers hnswlib-node fast-xml-parser

      - name: Create updater script
        run: |
          cat > update-data.js << 'EOL'
          // ... 此处省略 update-data.js 的完整内容，与您上一版本相同 ...
          // ... 请确保这里是您最新的、无格式错误、且包含所有新逻辑的脚本 ...
          EOL

      - name: Install dependencies
        run: npm install node-fetch@2 dotenv @xenova/transformers hnswlib-node fast-xml-parser

      - name: Create updater script
        run: |
          cat > update-data.js << 'EOL'
          // ---【修正点1】---: 模块加载方式统一修正
          require('dotenv').config();
          const fetch = require('node-fetch');
          const { HierarchicalNSW } = require('hnswlib-node');

          // --- 配置 ---
          const DEBUG_MODE = false; // 正式运行时请设为 false
          
          const GITHUB_TOKEN = process.env.GITHUB_TOKEN;
          const GITHUB_REPO = process.env.GITHUB_REPOSITORY;
          const PERPLEXITY_API_KEY = process.env.PERPLEXITY_API_KEY;
          
          const DATA_FILE = 'academic_data.json';
          const KEYWORD_HISTORY_FILE = 'daily_keyword_history.json';
          const FULL_KEYWORDS_FILE = 'keywords_full.json';
          const PROCESSED_URLS_FILE = 'processed_urls.json';

          const STAGING_AREA_LIMIT = 50;
          const TOP_N_TO_PROCESS = 6;
          const MAX_PAPERS_TO_KEEP = 100;
          const KEYWORD_HISTORY_WINDOW = 15;

          const DEFAULT_KEYWORDS = ["通信", "AI 6G", "Agent", "LLM", "语义通信"];
          const KEYWORDS_ENGLISH_MAPPING = { "通信": "Communications", "AI 6G": "AI 6G", "Agent": "Agent", "LLM": "Large Language Model", "语义通信": "Semantic Communications" };
          const EMBEDDING_MODEL = 'Xenova/all-MiniLM-L6-v2';
          const VECTOR_DIMENSIONS = 384;

          // --- 核心工具函数 ---
          async function fetchJsonFile(fileName, defaultValue = null) {
            try {
              const url = `https://api.github.com/repos/${GITHUB_REPO}/contents/${fileName}`;
              const headers = { 'Authorization': `token ${GITHUB_TOKEN}` };
              const response = await fetch(url, { headers });
              if (!response.ok) {
                if (response.status === 404) { console.log(`文件 ${fileName} 未找到，将使用默认值。`); return { content: defaultValue, sha: null }; }
                throw new Error(`GitHub API error for ${fileName}: ${response.status}`);
              }
              const data = await response.json();
              const content = Buffer.from(data.content, 'base64').toString('utf8');
              console.log(`✅ 成功获取文件 ${fileName}`);
              return { content: JSON.parse(content), sha: data.sha };
            } catch (error) {
              console.error(`获取文件 ${fileName} 时出错:`, error.message);
              return { content: defaultValue, sha: null };
            }
          }

          async function saveJsonFile(fileName, data, sha) {
            try {
              const url = `https://api.github.com/repos/${GITHUB_REPO}/contents/${fileName}`;
              const headers = { 'Authorization': `token ${GITHUB_TOKEN}`, 'Content-Type': 'application/json' };
              const content = JSON.stringify(data, null, 2);
              const encodedContent = Buffer.from(content).toString('base64');
              const body = { message: `更新数据文件: ${fileName}`, content: encodedContent, branch: 'main' };
              if (sha) body.sha = sha;
              const response = await fetch(url, { method: 'PUT', headers, body: JSON.stringify(body) });
              if (!response.ok) { const errorText = await response.text(); throw new Error(`保存到GitHub文件 ${fileName} 失败 (${response.status}): ${errorText}`); } // 【修正点1】
              console.log(`✅ 成功保存文件到 ${fileName}`);
            } catch (error) {
              console.error(`保存文件 ${fileName} 时出错:`, error);
              throw error;
            }
          }
          
          // ---【修正点2】---: 修正 queryPerplexityAndParseJson 函数
          async function queryPerplexityAndParseJson(prompt) {
              try {
                  if (!PERPLEXITY_API_KEY) throw new Error("Perplexity API key is missing");
                  const url = "https://api.perplexity.ai/chat/completions";
                  const response = await fetch(url, {
                    method: 'POST',
                    headers: { 'Authorization': `Bearer ${PERPLEXITY_API_KEY}`, 'Content-Type': 'application/json' },
                    body: JSON.stringify({ model: "sonar-pro", messages: [{ role: "user", content: prompt }] }) // 使用传入的 prompt
                  });
                  if (!response.ok) { const errorText = await response.text(); throw new Error(`Perplexity API error (${response.status}): ${errorText}`);}
                  const result = await response.json(); // 直接使用 response.json()
                  const content = result.choices[0].message.content;
                  const jsonMatch = content.match(/\{[\s\S]*\}/);
                  if (jsonMatch) return JSON.parse(jsonMatch[0]);
                  return null;
              } catch (e) {
                  console.error("解析Perplexity返回的JSON时出错:", e);
                  return null;
              }
          }

          // --- 拉取“大库”函数 ---
          async function fetchStagingPapers(limit = STAGING_AREA_LIMIT, maxRetries = 3) {
            const { XMLParser } = await import('fast-xml-parser');
            const coreTopicsQuery = '((all:"AI" AND all:"6G") OR all:"Semantic Communication")';
            const crossTopics = ["Agent", "Memory Management"];
            const crossTopicsQuery = crossTopics.map(t => `all:"${t}"`).join(' OR ');
            // const now = new Date();
            // const threeMonthsAgo = new Date();
            // threeMonthsAgo.setMonth(now.getMonth() - 1);
            // const formatDate = (date) => `${date.getFullYear()}${(date.getMonth() + 1).toString().padStart(2, '0')}${date.getDate().toString().padStart(2, '0')}`;
            // const dateFilter = `submittedDate:[${formatDate(threeMonthsAgo)}0000 TO ${formatDate(now)}2359]`;

            // ---【修改点】---: 调整日期范围为“过去24小时”
            const now = new Date();
            const yesterday = new Date();
            // 1. 计算出24小时前的精确时间点
            yesterday.setHours(now.getHours() - 24);

            // 2. 创建一个能格式化到分钟的帮助函数
            const formatDateTime = (date) => {
                const YYYY = date.getFullYear();
                const MM = (date.getMonth() + 1).toString().padStart(2, '0');
                const DD = date.getDate().toString().padStart(2, '0');
                const HH = date.getHours().toString().padStart(2, '0');
                const MIN = date.getMinutes().toString().padStart(2, '0');
                return `${YYYY}${MM}${DD}${HH}${MIN}`;
            };

            // 3. 构建精确到分钟的日期过滤器
            const dateFilter = `submittedDate:[${formatDateTime(yesterday)} TO ${formatDateTime(now)}]`;
            // --- 修改结束 ---

            // const searchQuery = `${coreTopicsQuery} AND (${crossTopicsQuery}) AND ${dateFilter}`;
            const searchQuery = `${coreTopicsQuery} AND ${dateFilter}`;
            const url = `http://export.arxiv.org/api/query?search_query=${encodeURIComponent(searchQuery)}&max_results=${limit}&sortBy=submittedDate&sortOrder=descending`;
            console.log(`向 arXiv API 查询 (请求 ${limit} 篇): ${searchQuery}`);
            for (let i = 0; i < maxRetries; i++) {
                try {
                    const response = await fetch(url);
                    if (response.status !== 200) throw new Error(`API 返回错误: ${response.status}`);
                    const xmlData = await response.text();
                    const parser = new XMLParser({ ignoreAttributes: false, attributeNamePrefix: "@_" });
                    const jsonData = parser.parse(xmlData);
                    if (!jsonData.feed || !jsonData.feed.entry) return [];
                    const entries = Array.isArray(jsonData.feed.entry) ? jsonData.feed.entry : [jsonData.feed.entry];
                    const papers = entries.map(entry => {
                        let authors = Array.isArray(entry.author) ? entry.author.map(a => a.name) : (entry.author ? [entry.author.name] : []);
                        const pdfLink = Array.isArray(entry.link) ? entry.link.find(l => l['@_title'] === 'pdf') : entry.link;
                        const paperId = pdfLink ? pdfLink['@_href'].split('/').pop().replace(/v\d+$/, '') : entry.id;
                        return { id: paperId, title: entry.title.replace(/\s+/g, ' ').trim(), url: entry.id, authors: authors, journal: 'arXiv', publicationDate: entry.published.split('T')[0], snippet: entry.summary.replace(/\s+/g, ' ').trim() };
                    });
                    console.log(`✅ 成功从 arXiv 获取并解析了 ${papers.length} 篇论文。`);
                    return papers;
                } catch (error) {
                    console.warn(`查询 arXiv 时出错 (第 ${i + 1} 次尝试):`, error.message);
                    if (i === maxRetries - 1) { console.error("已达到最大重试次数。"); return []; }
                    await new Promise(resolve => setTimeout(resolve, Math.pow(2, i) * 1000));
                }
            }
            return [];
          }

          // ---【修正点3】---: 重构为单篇论文精处理函数
          async function analyzeSinglePaper(paper, embedder, memoryIndex, memoryContent) {
            console.log(`🤔 思考: 正在精处理论文 "${paper.title}"`);
            
            // 步骤一：论文类型分类
            const classificationPrompt = `根据以下论文的标题和摘要，请判断这篇论文最接近哪种类型？只需从 ["实验/方法型", "综述/回顾型", "观点/立场型"] 中选择一个返回。请严格按照JSON格式返回，例如: {"paper_type": "实验/方法型"}\n\n标题: ${paper.title}\n摘要: ${paper.snippet}`;
            const classificationResult = await queryPerplexityAndParseJson(classificationPrompt);
            const paperType = classificationResult ? classificationResult.paper_type : "实验/方法型";
            console.log(`✅ 论文类型判断为: ${paperType}`);

            // 步骤二：相似性搜索
            let similarMemoriesPrompt = "";
            if (paper.snippet && memoryIndex.getCurrentCount() > 0) {
              const queryVector = await embedder(paper.snippet, { pooling: 'mean', normalize: true });
              const searchResult = memoryIndex.searchKnn(Array.from(queryVector.data), 2);
              if (searchResult.neighbors.length > 0) {
                const similarPapersContent = searchResult.neighbors.map(index => memoryContent[index]).filter(Boolean).map((mem, i) => `历史相关论文 ${i+1}:\n- 标题: ${mem.title}\n- 摘要: ${mem.snippet}`).join("\n\n");
                similarMemoriesPrompt = `作为一个拥有记忆的专家，请参考以下与当前论文主题相似的历史论文，进行对比和关联分析。\n--- 历史相似论文上下文 ---\n${similarPapersContent}\n--------------------------\n`;
              }
            }
            
            // 步骤三：根据类型选择分析框架
            let summarizationPrompt = '';
            switch (paperType) {
              case "综述/回顾型":
                console.log("📘 使用“综述/回顾型”分析框架...");
                summarizationPrompt = `请扮演一个专业的科研助理。${similarMemoriesPrompt}根据我提供的“综述/回顾型”论文的标题和摘要，请严格按照下面的JSON格式返回你的分析结果。\n\n### JSON格式要求:\n{"snippet": "...", "interpretation": {"survey_scope": "...", "taxonomy_or_structure": "...", "key_trends_and_insights": "...", "target_audience": "..."}, "paperKeywords": ["..."]}\n\n### 各字段生成要求:\n1. snippet: 生成一个约150字的“学术摘要”，客观概括其核心内容。\n2. interpretation: 提供“深入解读”，必须是一个包含以下四个键的JSON对象：\n   - survey_scope: 这篇综述覆盖了哪个具体研究领域？\n   - taxonomy_or_structure: 作者是如何对该领域的现有工作进行分类和组织的？\n   - key_trends_and_insights: 作者从回顾中总结出了哪些关键的技术发展趋势或未来洞见？\n   - target_audience: 这篇综述最适合哪类读者阅读？\n3. paperKeywords: 提取或生成5-7个核心关键词。\n\n### 待分析的论文信息：\n标题: ${paper.title}\n摘要: ${paper.snippet}`;
                break;
              default:
                console.log("🧪 使用“实验/方法型”分析框架...");
                summarizationPrompt = `请扮演一个专业的科研助理。${similarMemoriesPrompt}根据我提供的“实验/方法型”论文的标题和摘要，请严格按照下面的JSON格式返回你的分析结果。\n\n### JSON格式要求:\n{"snippet": "...", "interpretation": {"core_contribution": "...", "methodology_summary": "...", "performance_evaluation": "...", "inferred_application": "..."}, "paperKeywords": ["..."]}\n\n### 各字段生成要求:\n1. snippet: 生成一个约150字的“学术摘要”，客观概括其核心背景、方法和结论。\n2. interpretation: 提供“深入解读”，必须是一个包含以下四个键的JSON对象：\n   - core_contribution: 本文最关键的技术贡献、方法创新或观点突破是什么？\n   - methodology_summary: 其核心方法的技术原理是什么？\n   - performance_evaluation: 论文是如何评估其性能的？（如：数据集、基线模型、评估指标）\n   - inferred_application: 这项研究最直接的应用场景或目标用户是谁？\n3. paperKeywords: 提取或生成5-7个核心关键词。\n\n### 待分析的论文信息：\n标题: ${paper.title}\n摘要: ${paper.snippet}`;
                break;
            }

            const summaryData = await queryPerplexityAndParseJson(summarizationPrompt);
            if (!summaryData) { console.log(`👀 观察: 论文 "${paper.title}" 的摘要生成失败，已跳过。`); return null; }
            
            console.log(`👀 观察: 已成功生成论文 "${paper.title}" 的摘要和解读。`);
            return { id: paper.id, title: paper.title, url: paper.url, authors: paper.authors || [], affiliations: [], journal: paper.journal, publicationDate: paper.publicationDate, ...summaryData };
          }

          // --- 主函数 ---
          async function main() {
            try {
              const { pipeline } = await import('@xenova/transformers');
            
              // 1. 加载所有状态文件
              console.log("--- 步骤 1: 加载状态文件 ---");
              const { content: existingData, sha: dataSha } = await fetchJsonFile(DATA_FILE, { papers: [], keywords: { manual: DEFAULT_KEYWORDS, hot: [] } });
              const { content: keywordHistory, sha: historySha } = await fetchJsonFile(KEYWORD_HISTORY_FILE, []);
              const { sha: fullKeywordsSha } = await fetchJsonFile(FULL_KEYWORDS_FILE, []);
              const { content: processedUrlsData, sha: urlsSha } = await fetchJsonFile(PROCESSED_URLS_FILE, { processed_urls: [] });

              const manualKeywords = existingData.keywords.manual || [];
              const existingUrls = new Set(processedUrlsData.processed_urls);

              // 2. 构建分析记忆库 (小库)
              console.log("--- 步骤 2: 构建分析记忆库 (小库) ---");
              const memoryIndex = new HierarchicalNSW('l2', VECTOR_DIMENSIONS);
              const maxElements = existingData.papers.length > 0 ? existingData.papers.length : 1;
              memoryIndex.initIndex(maxElements);
              const memoryContent = {};
              const embedder = await pipeline('feature-extraction', EMBEDDING_MODEL);
              if (existingData.papers.length > 0) {
                console.log("🧠 正在向量化分析记忆库...");
                for (let i = 0; i < existingData.papers.length; i++) {
                    const paper = existingData.papers[i];
                    if (paper.snippet) {
                        const embedding = await embedder(paper.snippet, { pooling: 'mean', normalize: true });
                        memoryIndex.addPoint(Array.from(embedding.data), i);
                        memoryContent[i] = { title: paper.title, snippet: paper.snippet };
                    }
                }
              }
              console.log(`✅ 分析记忆库构建完成，包含 ${memoryIndex.getCurrentCount()} 条记忆。`);

              // 3. 拉取"大库"并去重
              console.log("--- 步骤 3: 从arXiv拉取“大库”并去重 ---");
              const stagingPapers = await fetchStagingPapers(STAGING_AREA_LIMIT);
              const newUniqueStagingPapers = stagingPapers.filter(p => p.url && !existingUrls.has(p.url));
              console.log(`从"大库"中发现 ${newUniqueStagingPapers.length} 篇全新论文。`);
              
              if (newUniqueStagingPapers.length === 0 && !DEBUG_MODE) {
                  console.log("没有可处理的全新论文，工作流正常结束。"); return;
              }

              // 4. 智能筛选
              console.log("--- 步骤 4: 智能筛选Top N篇论文 ---");
              const interestQuery = manualKeywords.join(' ');
              const queryVector = await embedder(interestQuery, { pooling: 'mean', normalize: true });
              const paperSimilarities = [];
              for(const paper of newUniqueStagingPapers) {
                  const paperVector = await embedder(paper.snippet, { pooling: 'mean', normalize: true });
                  const similarity = queryVector.data.reduce((sum, val, i) => sum + val * paperVector.data[i], 0);
                  paperSimilarities.push({ paper, similarity });
              }
              paperSimilarities.sort((a, b) => b.similarity - a.similarity);
              const topPapersToProcess = paperSimilarities.slice(0, TOP_N_TO_PROCESS).map(item => item.paper);
              console.log(`✅ 已筛选出 Top ${topPapersToProcess.length} 篇最相关的论文进行精处理。`);

              // 5. 精处理选出的论文
              console.log("--- 步骤 5: 对Top N篇论文进行精处理 ---");
              let processedPapers = [];
              if (DEBUG_MODE) {
                console.log("🚀 调试模式已开启，跳过所有精处理API调用。");
              } else {
                  for (const paper of topPapersToProcess) {
                      const processedPaper = await analyzeSinglePaper(paper, embedder, memoryIndex, memoryContent);
                      if (processedPaper) { processedPapers.push(processedPaper); }
                  }
              }
              
              if (processedPapers.length === 0 && !DEBUG_MODE) { console.log("精处理后没有可用的新论文，工作流结束。"); return; }
              if (processedPapers.length > 0) { console.log(`✅ 成功精处理了 ${processedPapers.length} 篇论文。`); }

              // 6. 更新关键词历史与热词
              console.log("--- 步骤 6: 更新关键词 ---");
              const todaysKeywords = processedPapers.flatMap(p => p.paperKeywords || []);
              if (todaysKeywords.length > 0) {
                  const today = new Date().toISOString().split('T')[0];
                  keywordHistory.push({ date: today, keywords: todaysKeywords });
                  while (keywordHistory.length > KEYWORD_HISTORY_WINDOW) { keywordHistory.shift(); }
                  await saveJsonFile(KEYWORD_HISTORY_FILE, keywordHistory, historySha);
              }
              const allKeywordsInWindow = keywordHistory.flatMap(day => day.keywords);
              const keywordFreq = allKeywordsInWindow.reduce((acc, k) => { acc[k] = (acc[k] || 0) + 1; return acc; }, {});
              const sortedKeywords = Object.entries(keywordFreq).sort((a, b) => b[1] - a[1]);
              const newHotKeywords = sortedKeywords.slice(0, 3).map(item => item[0]);
              console.log(`🔥 新的滚动热门关键词为: ${newHotKeywords.join(', ')}`);
              await saveJsonFile(FULL_KEYWORDS_FILE, sortedKeywords.map(([k, count]) => ({ keyword: k, count })), fullKeywordsSha);
              
              // 7. 更新永久去重URL列表
              console.log("--- 步骤 7: 更新永久去重URL列表 ---");
              const newUrls = processedPapers.map(p => p.url);
              processedUrlsData.processed_urls.push(...newUrls);
              await saveJsonFile(PROCESSED_URLS_FILE, processedUrlsData, urlsSha);

              // 8. 更新核心分析记忆库 (小库)
              console.log("--- 步骤 8: 更新核心分析记忆库 (小库) ---");
              const combinedPapers = [...processedPapers, ...existingData.papers];
              // ---【新增修改】---: 在裁剪前，按发表日期对所有论文进行降序排列
              // combinedPapers.sort((a, b) => b.publicationDate.localeCompare(a.publicationDate));
              const finalPapers = combinedPapers.slice(0, MAX_PAPERS_TO_KEEP);
              console.log(`将保留最新的 ${finalPapers.length} 篇论文到核心分析记忆库。`);
              const newData = { papers: finalPapers, lastUpdate: new Date().toISOString(), keywords: { manual: manualKeywords, hot: newHotKeywords }, lastAutoUpdateDate: new Date().toLocaleDateString('zh-CN', {timeZone: 'Asia/Shanghai'})};
              await saveJsonFile(DATA_FILE, newData, dataSha);
              
              console.log("🚀 所有数据更新成功，工作流结束。");

            } catch (error) {
              console.error("更新过程中出错:", error);
              process.exit(1);
            }
          }
          
          main();
          EOL

      - name: Run updater
        id: run_updater_step
        env:
          TRANSFORMERS_CACHE: ./.cache/huggingface/hub # 通过环境变量强制指定模型下载路径
          GITHUB_TOKEN: ${{ secrets.GH_PAT }}
          PERPLEXITY_API_KEY: ${{ secrets.PERPLEXITY_API_KEY }}
        run: node update-data.js

      #增加一个调试步骤，用于查看缓存目录的真实结构
      # - name: Debug - List Cache Directory Contents
      #   # 无论上一步是否成功都运行此步骤，以获取最多的调试信息
      #   if: always() 
      #   run: |
      #     echo "Listing contents of ./.cache/huggingface/hub directory..."
      #     ls -R ./.cache/huggingface/hub

      - name: Send email notification
        if: success()
        uses: dawidd6/action-send-mail@v5
        with:
          server_address: smtp.163.com
          server_port: 465
          secure: true
          username: 13937372851@163.com
          password: PCf7BnBBtXx9c6wk
          subject: 学术周报已更新 - ${{ github.repository }}
          body: |
            您好，
            您的学术周报已成功更新！新的学术论文已经添加到数据库中。
            您可以通过以下链接访问您的学术周报：
            https://shuhanliang.github.io/paper_summary_new/
            更新时间：${{ github.event.repository.updated_at }}
            仓库：${{ github.repository }}
            
            此邮件由GitHub Actions自动发送，请勿回复。
          to: blumanchu111@gmail.com, chenhui.a.ye@nokia-sbell.com
          from: 学术周报自动更新 <13937372851@163.com>
          nodemailerlog: true
          nodemailerdebug: true
