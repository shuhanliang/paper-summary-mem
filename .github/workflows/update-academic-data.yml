name: Update Academic Data

on:
  schedule:
    # 每个工作日（周一至周五）北京时间早上8:00运行（UTC时间0:00）
    # 格式: 分 时 日 月 周几 (0-6，0是周日)
    - cron: '0 0 * * 1-5'
  # 允许手动触发
  workflow_dispatch:

jobs:
  update-data:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'

      - name: Install dependencies
        run: npm install node-fetch@2 dotenv @xenova/transformers hnswlib-node

      - name: Create updater script
        run: |
          cat > update-data.js << 'EOL'
          require('dotenv').config();
          const fetch = require('node-fetch');
          // const { pipeline } = require('@xenova/transformers');
          const { HierarchicalNSW } = require('hnswlib-node');


          // --- 新增：调试模式开关 ---
          // 设置为 true 来跳过所有API调用，直接用已有的数据测试后续流程
          // 设置为 false 以正常调用 Semantic Scholar 和 Perplexity API
          const DEBUG_MODE = true;

          // --- 配置 ---
          const GITHUB_TOKEN = process.env.GITHUB_TOKEN;
          const GITHUB_REPO = process.env.GITHUB_REPOSITORY;
          const PERPLEXITY_API_KEY = process.env.PERPLEXITY_API_KEY;
          
          const DATA_FILE = 'academic_data.json';
          const KEYWORD_HISTORY_FILE = 'daily_keyword_history.json';
          const FULL_KEYWORDS_FILE = 'keywords_full.json';

          const DEFAULT_KEYWORDS = ["通信", "AI 6G", "Agent", "LLM", "语义通信"];
          const KEYWORDS_ENGLISH_MAPPING = {
            "通信": "Communications", "AI 6G": "AI 6G", "Agent": "Agent",
            "LLM": "Large Language Model", "语义通信": "Semantic Communications"
          };
          const EMBEDDING_MODEL = 'Xenova/all-MiniLM-L6-v2'; // 新增：定义嵌入模型
          const VECTOR_DIMENSIONS = 384; // 新增：嵌入模型的维度，all-MiniLM-L6-v2是384

          // --- 核心工具函数 ---

          // 从GitHub获取通用JSON文件
          async function fetchJsonFile(fileName, defaultValue = null) {
            try {
              const url = `https://api.github.com/repos/${GITHUB_REPO}/contents/${fileName}`;
              const headers = { 'Authorization': `token ${GITHUB_TOKEN}` };
              const response = await fetch(url, { headers });
              if (!response.ok) {
                if (response.status === 404) {
                  console.log(`文件 ${fileName} 未找到，将使用默认值。`);
                  return { content: defaultValue, sha: null };
                }
                throw new Error(`GitHub API error for ${fileName}: ${response.status}`);
              }
              const data = await response.json();
              const content = Buffer.from(data.content, 'base64').toString('utf8');
              console.log(`✅ 成功获取文件 ${fileName}`);
              return { content: JSON.parse(content), sha: data.sha };
            } catch (error) {
              console.error(`获取文件 ${fileName} 时出错:`, error.message);
              return { content: defaultValue, sha: null };
            }
          }

          // 保存通用JSON文件到GitHub
          async function saveJsonFile(fileName, data, sha) {
            try {
              const url = `https://api.github.com/repos/${GITHUB_REPO}/contents/${fileName}`;
              const headers = { 'Authorization': `token ${GITHUB_TOKEN}`, 'Content-Type': 'application/json' };
              const content = JSON.stringify(data, null, 2);
              const encodedContent = Buffer.from(content).toString('base64');
              const body = { message: `更新数据文件: ${fileName}`, content: encodedContent, branch: 'main' };
              if (sha) body.sha = sha;
              const response = await fetch(url, { method: 'PUT', headers, body: JSON.stringify(body) });
              if (!response.ok) {
                const errorText = await response.text();
                throw new Error(`保存到GitHub文件 ${fileName} 失败 (${response.status}): ${errorText}`);
              }
              console.log(`✅ 成功保存文件到 ${fileName}`);
            } catch (error) {
              console.error(`保存文件 ${fileName} 时出错:`, error);
              throw error;
            }
          }
          
          // 新增：调用Semantic Scholar API的函数（包含指数退避重试逻辑）
          async function searchSemanticScholar(keywords, limit = 5, maxRetries = 3) {
              console.log(`向 Semantic Scholar 查询关键词: ${keywords.join(' ')}`);
              const query = encodeURIComponent(keywords.join(' '));
              const fields = 'paperId,url,title,abstract,authors,year,journal';
              const url = `https://api.semanticscholar.org/graph/v1/paper/search?query=${query}&limit=${limit}&fields=${fields}`;

              for (let i = 0; i < maxRetries; i++) {
                  try {
                      const response = await fetch(url);
                      if (response.status === 429 || response.status >= 500) {
                          throw new Error(`可重试的API错误: ${response.status}`);
                      }
                      if (!response.ok) {
                          console.error(`不可重试的API错误: ${response.status}`);
                          return [];
                      }
                      const data = await response.json();
                      console.log("✅ 成功从 Semantic Scholar 获取数据。");
                      return data.data || [];
                  } catch (error) {
                      console.warn(`查询 Semantic Scholar 时出错 (第 ${i + 1} 次尝试):`, error.message);
                      if (i === maxRetries - 1) {
                        console.error("已达到最大重试次数，放弃请求。");
                        return [];
                      }
                      const delay = Math.pow(2, i) * 1000 + (Math.random() * 500);
                      console.log(`将在 ${Math.round(delay / 1000)} 秒后重试...`);
                      await new Promise(resolve => setTimeout(resolve, delay));
                    }
                }
                return [];
            }

          // 调用Perplexity API的函数
          async function querySonarPro(question) {
            if (!PERPLEXITY_API_KEY) throw new Error("Perplexity API key is missing");
            const url = "https://api.perplexity.ai/chat/completions";
            const response = await fetch(url, {
              method: 'POST',
              headers: { 'Authorization': `Bearer ${PERPLEXITY_API_KEY}`, 'Content-Type': 'application/json' },
              body: JSON.stringify({ model: "sonar-pro", messages: [{ role: "user", content: question }] })
            });
            if (!response.ok) {
              const errorText = await response.text();
              throw new Error(`Perplexity API error (${response.status}): ${errorText}`);
            }
            return await response.json();
          }
          
          // 解析Perplexity返回的JSON
          async function queryPerplexityAndParseJson(prompt) {
              try {
                  const result = await querySonarPro(prompt);
                  const content = result.choices[0].message.content;
                  const jsonMatch = content.match(/\{[\s\S]*\}/);
                  if (jsonMatch) return JSON.parse(jsonMatch[0]);
                  return null;
              } catch (e) {
                  console.error("解析Perplexity返回的JSON时出错:", e);
                  return null;
              }
          }

          // Agent核心逻辑：获取并处理论文 -- 修改：集成向量搜索
          async function runAgentAndFetchPapers(keywords, existingPaperUrls, embedder, memoryIndex, memoryContent) {
              console.log("🤔 思考: 需要寻找新论文。");
              console.log("🎬 行动: 调用 searchSemanticScholar");
              const papersFromScholar = await searchSemanticScholar(keywords);
              console.log(`👀 观察: 从Semantic Scholar获得 ${papersFromScholar.length} 篇论文。`);

              const newUniquePapersFromScholar = papersFromScholar.filter(p => p.url && !existingPaperUrls.has(p.url));
              console.log(`筛选后得到 ${newUniquePapersFromScholar.length} 篇全新论文需要处理。`);

              const processedPapers = [];
              for (const paper of newUniquePapersFromScholar) {
                  console.log(`🤔 思考: 正在处理论文 "${paper.title}"`);

                  // --- 新增：向量化并搜索相似记忆 ---
                  let similarMemoriesPrompt = "";
                  if (paper.abstract && memoryIndex.getCurrentCount() > 0) {
                      console.log("🎬 行动: 向量化新论文摘要并搜索记忆库...");
                      const queryVector = await embedder(paper.abstract, { pooling: 'mean', normalize: true });
                      const searchResult = memoryIndex.searchKnn(queryVector.data, 2); // 寻找最相似的2篇
            
                      if (searchResult.neighbors.length > 0) {
                        const similarPapersContent = searchResult.neighbors
                            .map(index => memoryContent[index])
                            .filter(Boolean)
                            .map((mem, i) => `历史相关论文 ${i+1}:\n- 标题: ${mem.title}\n- 摘要: ${mem.snippet}`)
                            .join("\n\n");

                            similarMemoriesPrompt = `
                              作为一个拥有记忆的专家，请参考以下与当前论文主题相似的历史论文，进行对比和关联分析，从而提供更有深度的解读。
                              --- 历史相似论文上下文 ---
                              ${similarPapersContent}
                              --------------------------
                              `;
                        }
                    }



                  // --- 修改：构建包含“相似记忆”的Prompt ---
                  const summarizationPrompt = `
                      请扮演一个专业的科研助理。
                      ${similarMemoriesPrompt}
                      根据以下真实论文的标题和摘要，为我生成中文的“摘要（snippet）”、“深入解读（interpretation）”和“关键词（paperKeywords）”。
                      请严格按照JSON格式返回，不要包含任何额外的解释或开头语。
                      格式: {"snippet": "...", "interpretation": "...", "paperKeywords": ["keyword1", "keyword2"]}

                      论文标题: ${paper.title}
                      论文摘要 (Abstract): ${paper.abstract}
                  `;
                  console.log("🎬 行动: 调用 Perplexity API 进行总结...");
                  const summaryData = await queryPerplexityAndParseJson(summarizationPrompt); 

                  if (summaryData) {
                      console.log(`👀 观察: 已成功生成论文 "${paper.title}" 的摘要和解读。`);
                      processedPapers.push({
                          id: paper.paperId || `scholar-${Date.now()}`,
                          title: paper.title,
                          url: paper.url,
                          authors: paper.authors ? paper.authors.map(a => a.name) : [],
                          affiliations: [],
                          journal: paper.journal && paper.journal.name ? paper.journal.name : "N/A",
                          publicationDate: paper.year,
                          ...summaryData
                      });
                  } else {
                      console.log(`👀 观察: 论文 "${paper.title}" 的摘要生成失败，已跳过。`);
                  }
              }
              return processedPapers;
          }


          // --- 主函数 ---
          async function main() {
            // --- 新增：使用动态 import() 加载ESM模块 ---
            const { pipeline } = await import('@xenova/transformers');
            // --- 修正：使用最终正确的命名导出语法 ---
            // const { HierarchicalNSW } = await import('hnswlib-node');

            try {
              // --- 步骤 1: 加载所有状态文件 ---
              const { content: existingData, sha: dataSha } = await fetchJsonFile(DATA_FILE, { keywords: { manual: DEFAULT_KEYWORDS, hot: [] }, papers: [] });
              const { content: keywordHistory, sha: historySha } = await fetchJsonFile(KEYWORD_HISTORY_FILE, []);
              const { sha: fullKeywordsSha } = await fetchJsonFile(FULL_KEYWORDS_FILE, []);

              const manualKeywords = existingData.keywords.manual || DEFAULT_KEYWORDS;
              const hotKeywords = existingData.keywords.hot || [];
              const searchKeywords = Array.from(new Set([...manualKeywords, ...hotKeywords]));
              const existingPaperUrls = new Set((existingData.papers || []).map(p => p.url).filter(url => url));

              // ------ 新增：初始化嵌入模型和向量记忆库 ---
              console.log("🚀 初始化文本嵌入模型...");
              const embedder = await pipeline('feature-extraction', EMBEDDING_MODEL);

              console.log("🧠 正在从 academic_data.json 构建向量记忆库...");
              const memoryIndex = new HierarchicalNSW('l2', VECTOR_DIMENSIONS);

              // 如果initIndex需要一个非零参数，我们需要确保即使在没有论文的情况下也能工作
              const maxElements = existingData.papers.length > 0 ? existingData.papers.length : 1;
              memoryIndex.initIndex(maxElements);

              // 【修改】声明 memoryContent 用于存储原文
              const memoryContent = {}; // 【修改】

              if (existingData.papers.length > 0) {
                for (let i = 0; i < existingData.papers.length; i++) {
                    const paper = existingData.papers[i];
                    if (paper.snippet) {
                      const embedding = await embedder(paper.snippet, { pooling: 'mean', normalize: true });
                      memoryIndex.addPoint(embedding.data, i);
                      memoryContent[i] = { title: paper.title, snippet: paper.snippet };
                    }
                }
              }
              console.log(`✅ 向量记忆库构建完成，包含 ${memoryIndex.getCurrentCount()} 条记忆。`);



              // --- 步骤 2: 运行Agent获取新论文 (含Debug模式) ---
              let newPapers = [];
              if (DEBUG_MODE) {
                  console.log("🚀 调试模式已开启，跳过所有API调用。");
                  // 在调试模式下，我们假装没有获取到新论文，以便测试后续的关键词统计和文件保存逻辑
              } else {
                  // 修改：传入记忆库相关参数
                  newPapers = await runAgentAndFetchPapers(searchKeywords, existingPaperUrls, embedder, memoryIndex, memoryContent);
              }

              if (!DEBUG_MODE && newPapers.length === 0) {
                  console.log("没有获取到需要处理的新论文，工作流正常结束。");
                  return;
              }
              if (!DEBUG_MODE) {
                  console.log(`✅ 成功处理了 ${newPapers.length} 篇新论文。`);
              }

              // --- 步骤 3: 更新10日滚动热词 ---
              const todaysKeywords = newPapers.flatMap(p => p.paperKeywords || []);
              if (todaysKeywords.length > 0) {
                  const today = new Date().toISOString().split('T')[0];
                  keywordHistory.push({ date: today, keywords: todaysKeywords });
                  while (keywordHistory.length > 15) {
                      keywordHistory.shift();
                  }
                  await saveJsonFile(KEYWORD_HISTORY_FILE, keywordHistory, historySha);
              }

              const allKeywordsInWindow = keywordHistory.flatMap(day => day.keywords);
              const keywordFreq = allKeywordsInWindow.reduce((acc, k) => { acc[k] = (acc[k] || 0) + 1; return acc; }, {});
              const sortedKeywords = Object.entries(keywordFreq).sort((a, b) => b[1] - a[1]);
              const newHotKeywords = sortedKeywords.slice(0, 3).map(item => item[0]);
              console.log(`🔥 新的滚动热门关键词为: ${newHotKeywords.join(', ')}`);

              // --- 步骤 4: 更新全景关键词文件 ---
              const sortedKeywordsFull = sortedKeywords.map(([k, count]) => ({ keyword: k, count }));
              await saveJsonFile(FULL_KEYWORDS_FILE, sortedKeywordsFull, fullKeywordsSha);
              
              // --- 步骤 5: 合并与归档论文 ---
              const combinedPapers = [...newPapers, ...(existingData.papers || [])];
              const MAX_PAPERS_TO_KEEP = 50;
              const finalPapers = combinedPapers.slice(0, MAX_PAPERS_TO_KEEP);
              console.log(`将保留最新的 ${finalPapers.length} 篇论文。`);

              // --- 步骤 6: 准备并保存最终的主数据文件 ---
              const newData = {
                papers: finalPapers,
                lastUpdate: new Date().toISOString(),
                keywords: {
                  manual: manualKeywords,
                  hot: newHotKeywords
                },
                lastAutoUpdateDate: new Date().toLocaleDateString('zh-CN', {timeZone: 'Asia/Shanghai'})
              };
              
              await saveJsonFile(DATA_FILE, newData, dataSha);
              console.log("🚀 所有数据更新成功，工作流结束。");
              
            } catch (error) {
              console.error("更新过程中出错:", error);
              process.exit(1);
            }
          }
          
          main();
          EOL

      - name: Run updater
        env:
          GITHUB_TOKEN: ${{ secrets.GH_PAT }}
          PERPLEXITY_API_KEY: ${{ secrets.PERPLEXITY_API_KEY }}
        run: node update-data.js

      - name: Send email notification
        if: success()
        uses: dawidd6/action-send-mail@v5
        with:
          server_address: smtp.163.com
          server_port: 465
          secure: true
          username: 13937372851@163.com
          password: PCf7BnBBtXx9c6wk
          subject: 学术周报已更新 - ${{ github.repository }}
          body: |
            您好，
            
            您的学术周报已成功更新！新的学术论文已经添加到数据库中。
            
            您可以通过以下链接访问您的学术周报：
            https://shuhanliang.github.io/paper_summary/
            
            更新时间：${{ github.event.repository.updated_at }}
            仓库：${{ github.repository }}
            
            此邮件由GitHub Actions自动发送，请勿回复。
          to: blumanchu111@gmail.com, chenhui.a.ye@nokia-sbell.com
          from: 学术周报自动更新 <13937372851@163.com>
          nodemailerlog: true
          nodemailerdebug: true
