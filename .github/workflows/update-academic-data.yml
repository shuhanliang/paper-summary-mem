name: Update Academic Data

on:
  schedule:
    # æ¯ä¸ªå·¥ä½œæ—¥ï¼ˆå‘¨ä¸€è‡³å‘¨äº”ï¼‰åŒ—äº¬æ—¶é—´æ—©ä¸Š8:00è¿è¡Œï¼ˆUTCæ—¶é—´0:00ï¼‰
    # æ ¼å¼: åˆ† æ—¶ æ—¥ æœˆ å‘¨å‡  (0-6ï¼Œ0æ˜¯å‘¨æ—¥)
    - cron: '0 0 * * 1-5'
  # å…è®¸æ‰‹åŠ¨è§¦å‘
  workflow_dispatch:

jobs:
  update-data:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'

      - name: Install dependencies
        run: npm install node-fetch@2 dotenv @xenova/transformers hnswlib-node

      - name: Create updater script
        run: |
          cat > update-data.js << 'EOL'
          require('dotenv').config();
          const fetch = require('node-fetch');
          // const { pipeline } = require('@xenova/transformers');
          const { HierarchicalNSW } = require('hnswlib-node');


          // --- æ–°å¢ï¼šè°ƒè¯•æ¨¡å¼å¼€å…³ ---
          // è®¾ç½®ä¸º true æ¥è·³è¿‡æ‰€æœ‰APIè°ƒç”¨ï¼Œç›´æ¥ç”¨å·²æœ‰çš„æ•°æ®æµ‹è¯•åç»­æµç¨‹
          // è®¾ç½®ä¸º false ä»¥æ­£å¸¸è°ƒç”¨ Semantic Scholar å’Œ Perplexity API
          const DEBUG_MODE = true;

          // --- é…ç½® ---
          const GITHUB_TOKEN = process.env.GITHUB_TOKEN;
          const GITHUB_REPO = process.env.GITHUB_REPOSITORY;
          const PERPLEXITY_API_KEY = process.env.PERPLEXITY_API_KEY;
          
          const DATA_FILE = 'academic_data.json';
          const KEYWORD_HISTORY_FILE = 'daily_keyword_history.json';
          const FULL_KEYWORDS_FILE = 'keywords_full.json';

          const DEFAULT_KEYWORDS = ["é€šä¿¡", "AI 6G", "Agent", "LLM", "è¯­ä¹‰é€šä¿¡"];
          const KEYWORDS_ENGLISH_MAPPING = {
            "é€šä¿¡": "Communications", "AI 6G": "AI 6G", "Agent": "Agent",
            "LLM": "Large Language Model", "è¯­ä¹‰é€šä¿¡": "Semantic Communications"
          };
          const EMBEDDING_MODEL = 'Xenova/all-MiniLM-L6-v2'; // æ–°å¢ï¼šå®šä¹‰åµŒå…¥æ¨¡å‹
          const VECTOR_DIMENSIONS = 384; // æ–°å¢ï¼šåµŒå…¥æ¨¡å‹çš„ç»´åº¦ï¼Œall-MiniLM-L6-v2æ˜¯384

          // --- æ ¸å¿ƒå·¥å…·å‡½æ•° ---

          // ä»GitHubè·å–é€šç”¨JSONæ–‡ä»¶
          async function fetchJsonFile(fileName, defaultValue = null) {
            try {
              const url = `https://api.github.com/repos/${GITHUB_REPO}/contents/${fileName}`;
              const headers = { 'Authorization': `token ${GITHUB_TOKEN}` };
              const response = await fetch(url, { headers });
              if (!response.ok) {
                if (response.status === 404) {
                  console.log(`æ–‡ä»¶ ${fileName} æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ã€‚`);
                  return { content: defaultValue, sha: null };
                }
                throw new Error(`GitHub API error for ${fileName}: ${response.status}`);
              }
              const data = await response.json();
              const content = Buffer.from(data.content, 'base64').toString('utf8');
              console.log(`âœ… æˆåŠŸè·å–æ–‡ä»¶ ${fileName}`);
              return { content: JSON.parse(content), sha: data.sha };
            } catch (error) {
              console.error(`è·å–æ–‡ä»¶ ${fileName} æ—¶å‡ºé”™:`, error.message);
              return { content: defaultValue, sha: null };
            }
          }

          // ä¿å­˜é€šç”¨JSONæ–‡ä»¶åˆ°GitHub
          async function saveJsonFile(fileName, data, sha) {
            try {
              const url = `https://api.github.com/repos/${GITHUB_REPO}/contents/${fileName}`;
              const headers = { 'Authorization': `token ${GITHUB_TOKEN}`, 'Content-Type': 'application/json' };
              const content = JSON.stringify(data, null, 2);
              const encodedContent = Buffer.from(content).toString('base64');
              const body = { message: `æ›´æ–°æ•°æ®æ–‡ä»¶: ${fileName}`, content: encodedContent, branch: 'main' };
              if (sha) body.sha = sha;
              const response = await fetch(url, { method: 'PUT', headers, body: JSON.stringify(body) });
              if (!response.ok) {
                const errorText = await response.text();
                throw new Error(`ä¿å­˜åˆ°GitHubæ–‡ä»¶ ${fileName} å¤±è´¥ (${response.status}): ${errorText}`);
              }
              console.log(`âœ… æˆåŠŸä¿å­˜æ–‡ä»¶åˆ° ${fileName}`);
            } catch (error) {
              console.error(`ä¿å­˜æ–‡ä»¶ ${fileName} æ—¶å‡ºé”™:`, error);
              throw error;
            }
          }
          
          // æ–°å¢ï¼šè°ƒç”¨Semantic Scholar APIçš„å‡½æ•°ï¼ˆåŒ…å«æŒ‡æ•°é€€é¿é‡è¯•é€»è¾‘ï¼‰
          async function searchSemanticScholar(keywords, limit = 5, maxRetries = 3) {
              console.log(`å‘ Semantic Scholar æŸ¥è¯¢å…³é”®è¯: ${keywords.join(' ')}`);
              const query = encodeURIComponent(keywords.join(' '));
              const fields = 'paperId,url,title,abstract,authors,year,journal';
              const url = `https://api.semanticscholar.org/graph/v1/paper/search?query=${query}&limit=${limit}&fields=${fields}`;

              for (let i = 0; i < maxRetries; i++) {
                  try {
                      const response = await fetch(url);
                      if (response.status === 429 || response.status >= 500) {
                          throw new Error(`å¯é‡è¯•çš„APIé”™è¯¯: ${response.status}`);
                      }
                      if (!response.ok) {
                          console.error(`ä¸å¯é‡è¯•çš„APIé”™è¯¯: ${response.status}`);
                          return [];
                      }
                      const data = await response.json();
                      console.log("âœ… æˆåŠŸä» Semantic Scholar è·å–æ•°æ®ã€‚");
                      return data.data || [];
                  } catch (error) {
                      console.warn(`æŸ¥è¯¢ Semantic Scholar æ—¶å‡ºé”™ (ç¬¬ ${i + 1} æ¬¡å°è¯•):`, error.message);
                      if (i === maxRetries - 1) {
                        console.error("å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒè¯·æ±‚ã€‚");
                        return [];
                      }
                      const delay = Math.pow(2, i) * 1000 + (Math.random() * 500);
                      console.log(`å°†åœ¨ ${Math.round(delay / 1000)} ç§’åé‡è¯•...`);
                      await new Promise(resolve => setTimeout(resolve, delay));
                    }
                }
                return [];
            }

          // è°ƒç”¨Perplexity APIçš„å‡½æ•°
          async function querySonarPro(question) {
            if (!PERPLEXITY_API_KEY) throw new Error("Perplexity API key is missing");
            const url = "https://api.perplexity.ai/chat/completions";
            const response = await fetch(url, {
              method: 'POST',
              headers: { 'Authorization': `Bearer ${PERPLEXITY_API_KEY}`, 'Content-Type': 'application/json' },
              body: JSON.stringify({ model: "sonar-pro", messages: [{ role: "user", content: question }] })
            });
            if (!response.ok) {
              const errorText = await response.text();
              throw new Error(`Perplexity API error (${response.status}): ${errorText}`);
            }
            return await response.json();
          }
          
          // è§£æPerplexityè¿”å›çš„JSON
          async function queryPerplexityAndParseJson(prompt) {
              try {
                  const result = await querySonarPro(prompt);
                  const content = result.choices[0].message.content;
                  const jsonMatch = content.match(/\{[\s\S]*\}/);
                  if (jsonMatch) return JSON.parse(jsonMatch[0]);
                  return null;
              } catch (e) {
                  console.error("è§£æPerplexityè¿”å›çš„JSONæ—¶å‡ºé”™:", e);
                  return null;
              }
          }

          // Agentæ ¸å¿ƒé€»è¾‘ï¼šè·å–å¹¶å¤„ç†è®ºæ–‡ -- ä¿®æ”¹ï¼šé›†æˆå‘é‡æœç´¢
          async function runAgentAndFetchPapers(keywords, existingPaperUrls, embedder, memoryIndex, memoryContent) {
              console.log("ğŸ¤” æ€è€ƒ: éœ€è¦å¯»æ‰¾æ–°è®ºæ–‡ã€‚");
              console.log("ğŸ¬ è¡ŒåŠ¨: è°ƒç”¨ searchSemanticScholar");
              const papersFromScholar = await searchSemanticScholar(keywords);
              console.log(`ğŸ‘€ è§‚å¯Ÿ: ä»Semantic Scholarè·å¾— ${papersFromScholar.length} ç¯‡è®ºæ–‡ã€‚`);

              const newUniquePapersFromScholar = papersFromScholar.filter(p => p.url && !existingPaperUrls.has(p.url));
              console.log(`ç­›é€‰åå¾—åˆ° ${newUniquePapersFromScholar.length} ç¯‡å…¨æ–°è®ºæ–‡éœ€è¦å¤„ç†ã€‚`);

              const processedPapers = [];
              for (const paper of newUniquePapersFromScholar) {
                  console.log(`ğŸ¤” æ€è€ƒ: æ­£åœ¨å¤„ç†è®ºæ–‡ "${paper.title}"`);

                  // --- æ–°å¢ï¼šå‘é‡åŒ–å¹¶æœç´¢ç›¸ä¼¼è®°å¿† ---
                  let similarMemoriesPrompt = "";
                  if (paper.abstract && memoryIndex.getCurrentCount() > 0) {
                      console.log("ğŸ¬ è¡ŒåŠ¨: å‘é‡åŒ–æ–°è®ºæ–‡æ‘˜è¦å¹¶æœç´¢è®°å¿†åº“...");
                      const queryVector = await embedder(paper.abstract, { pooling: 'mean', normalize: true });
                      const searchResult = memoryIndex.searchKnn(queryVector.data, 2); // å¯»æ‰¾æœ€ç›¸ä¼¼çš„2ç¯‡
            
                      if (searchResult.neighbors.length > 0) {
                        const similarPapersContent = searchResult.neighbors
                            .map(index => memoryContent[index])
                            .filter(Boolean)
                            .map((mem, i) => `å†å²ç›¸å…³è®ºæ–‡ ${i+1}:\n- æ ‡é¢˜: ${mem.title}\n- æ‘˜è¦: ${mem.snippet}`)
                            .join("\n\n");

                            similarMemoriesPrompt = `
                              ä½œä¸ºä¸€ä¸ªæ‹¥æœ‰è®°å¿†çš„ä¸“å®¶ï¼Œè¯·å‚è€ƒä»¥ä¸‹ä¸å½“å‰è®ºæ–‡ä¸»é¢˜ç›¸ä¼¼çš„å†å²è®ºæ–‡ï¼Œè¿›è¡Œå¯¹æ¯”å’Œå…³è”åˆ†æï¼Œä»è€Œæä¾›æ›´æœ‰æ·±åº¦çš„è§£è¯»ã€‚
                              --- å†å²ç›¸ä¼¼è®ºæ–‡ä¸Šä¸‹æ–‡ ---
                              ${similarPapersContent}
                              --------------------------
                              `;
                        }
                    }



                  // --- ä¿®æ”¹ï¼šæ„å»ºåŒ…å«â€œç›¸ä¼¼è®°å¿†â€çš„Prompt ---
                  const summarizationPrompt = `
                      è¯·æ‰®æ¼”ä¸€ä¸ªä¸“ä¸šçš„ç§‘ç ”åŠ©ç†ã€‚
                      ${similarMemoriesPrompt}
                      æ ¹æ®ä»¥ä¸‹çœŸå®è®ºæ–‡çš„æ ‡é¢˜å’Œæ‘˜è¦ï¼Œä¸ºæˆ‘ç”Ÿæˆä¸­æ–‡çš„â€œæ‘˜è¦ï¼ˆsnippetï¼‰â€ã€â€œæ·±å…¥è§£è¯»ï¼ˆinterpretationï¼‰â€å’Œâ€œå…³é”®è¯ï¼ˆpaperKeywordsï¼‰â€ã€‚
                      è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–å¼€å¤´è¯­ã€‚
                      æ ¼å¼: {"snippet": "...", "interpretation": "...", "paperKeywords": ["keyword1", "keyword2"]}

                      è®ºæ–‡æ ‡é¢˜: ${paper.title}
                      è®ºæ–‡æ‘˜è¦ (Abstract): ${paper.abstract}
                  `;
                  console.log("ğŸ¬ è¡ŒåŠ¨: è°ƒç”¨ Perplexity API è¿›è¡Œæ€»ç»“...");
                  const summaryData = await queryPerplexityAndParseJson(summarizationPrompt); 

                  if (summaryData) {
                      console.log(`ğŸ‘€ è§‚å¯Ÿ: å·²æˆåŠŸç”Ÿæˆè®ºæ–‡ "${paper.title}" çš„æ‘˜è¦å’Œè§£è¯»ã€‚`);
                      processedPapers.push({
                          id: paper.paperId || `scholar-${Date.now()}`,
                          title: paper.title,
                          url: paper.url,
                          authors: paper.authors ? paper.authors.map(a => a.name) : [],
                          affiliations: [],
                          journal: paper.journal && paper.journal.name ? paper.journal.name : "N/A",
                          publicationDate: paper.year,
                          ...summaryData
                      });
                  } else {
                      console.log(`ğŸ‘€ è§‚å¯Ÿ: è®ºæ–‡ "${paper.title}" çš„æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œå·²è·³è¿‡ã€‚`);
                  }
              }
              return processedPapers;
          }


          // --- ä¸»å‡½æ•° ---
          async function main() {
            // --- æ–°å¢ï¼šä½¿ç”¨åŠ¨æ€ import() åŠ è½½ESMæ¨¡å— ---
            const { pipeline } = await import('@xenova/transformers');
            // --- ä¿®æ­£ï¼šä½¿ç”¨æœ€ç»ˆæ­£ç¡®çš„å‘½åå¯¼å‡ºè¯­æ³• ---
            // const { HierarchicalNSW } = await import('hnswlib-node');

            try {
              // --- æ­¥éª¤ 1: åŠ è½½æ‰€æœ‰çŠ¶æ€æ–‡ä»¶ ---
              const { content: existingData, sha: dataSha } = await fetchJsonFile(DATA_FILE, { keywords: { manual: DEFAULT_KEYWORDS, hot: [] }, papers: [] });
              const { content: keywordHistory, sha: historySha } = await fetchJsonFile(KEYWORD_HISTORY_FILE, []);
              const { sha: fullKeywordsSha } = await fetchJsonFile(FULL_KEYWORDS_FILE, []);

              const manualKeywords = existingData.keywords.manual || DEFAULT_KEYWORDS;
              const hotKeywords = existingData.keywords.hot || [];
              const searchKeywords = Array.from(new Set([...manualKeywords, ...hotKeywords]));
              const existingPaperUrls = new Set((existingData.papers || []).map(p => p.url).filter(url => url));

              // ------ æ–°å¢ï¼šåˆå§‹åŒ–åµŒå…¥æ¨¡å‹å’Œå‘é‡è®°å¿†åº“ ---
              console.log("ğŸš€ åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹...");
              const embedder = await pipeline('feature-extraction', EMBEDDING_MODEL);

              console.log("ğŸ§  æ­£åœ¨ä» academic_data.json æ„å»ºå‘é‡è®°å¿†åº“...");
              const memoryIndex = new HierarchicalNSW('l2', VECTOR_DIMENSIONS);

              // å¦‚æœinitIndexéœ€è¦ä¸€ä¸ªéé›¶å‚æ•°ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿å³ä½¿åœ¨æ²¡æœ‰è®ºæ–‡çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å·¥ä½œ
              const maxElements = existingData.papers.length > 0 ? existingData.papers.length : 1;
              memoryIndex.initIndex(maxElements);

              // ã€ä¿®æ”¹ã€‘å£°æ˜ memoryContent ç”¨äºå­˜å‚¨åŸæ–‡
              const memoryContent = {}; // ã€ä¿®æ”¹ã€‘

              if (existingData.papers.length > 0) {
                for (let i = 0; i < existingData.papers.length; i++) {
                    const paper = existingData.papers[i];
                    if (paper.snippet) {
                      const embedding = await embedder(paper.snippet, { pooling: 'mean', normalize: true });
                      memoryIndex.addPoint(embedding.data, i);
                      memoryContent[i] = { title: paper.title, snippet: paper.snippet };
                    }
                }
              }
              console.log(`âœ… å‘é‡è®°å¿†åº“æ„å»ºå®Œæˆï¼ŒåŒ…å« ${memoryIndex.getCurrentCount()} æ¡è®°å¿†ã€‚`);



              // --- æ­¥éª¤ 2: è¿è¡ŒAgentè·å–æ–°è®ºæ–‡ (å«Debugæ¨¡å¼) ---
              let newPapers = [];
              if (DEBUG_MODE) {
                  console.log("ğŸš€ è°ƒè¯•æ¨¡å¼å·²å¼€å¯ï¼Œè·³è¿‡æ‰€æœ‰APIè°ƒç”¨ã€‚");
                  // åœ¨è°ƒè¯•æ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬å‡è£…æ²¡æœ‰è·å–åˆ°æ–°è®ºæ–‡ï¼Œä»¥ä¾¿æµ‹è¯•åç»­çš„å…³é”®è¯ç»Ÿè®¡å’Œæ–‡ä»¶ä¿å­˜é€»è¾‘
              } else {
                  // ä¿®æ”¹ï¼šä¼ å…¥è®°å¿†åº“ç›¸å…³å‚æ•°
                  newPapers = await runAgentAndFetchPapers(searchKeywords, existingPaperUrls, embedder, memoryIndex, memoryContent);
              }

              if (!DEBUG_MODE && newPapers.length === 0) {
                  console.log("æ²¡æœ‰è·å–åˆ°éœ€è¦å¤„ç†çš„æ–°è®ºæ–‡ï¼Œå·¥ä½œæµæ­£å¸¸ç»“æŸã€‚");
                  return;
              }
              if (!DEBUG_MODE) {
                  console.log(`âœ… æˆåŠŸå¤„ç†äº† ${newPapers.length} ç¯‡æ–°è®ºæ–‡ã€‚`);
              }

              // --- æ­¥éª¤ 3: æ›´æ–°10æ—¥æ»šåŠ¨çƒ­è¯ ---
              const todaysKeywords = newPapers.flatMap(p => p.paperKeywords || []);
              if (todaysKeywords.length > 0) {
                  const today = new Date().toISOString().split('T')[0];
                  keywordHistory.push({ date: today, keywords: todaysKeywords });
                  while (keywordHistory.length > 15) {
                      keywordHistory.shift();
                  }
                  await saveJsonFile(KEYWORD_HISTORY_FILE, keywordHistory, historySha);
              }

              const allKeywordsInWindow = keywordHistory.flatMap(day => day.keywords);
              const keywordFreq = allKeywordsInWindow.reduce((acc, k) => { acc[k] = (acc[k] || 0) + 1; return acc; }, {});
              const sortedKeywords = Object.entries(keywordFreq).sort((a, b) => b[1] - a[1]);
              const newHotKeywords = sortedKeywords.slice(0, 3).map(item => item[0]);
              console.log(`ğŸ”¥ æ–°çš„æ»šåŠ¨çƒ­é—¨å…³é”®è¯ä¸º: ${newHotKeywords.join(', ')}`);

              // --- æ­¥éª¤ 4: æ›´æ–°å…¨æ™¯å…³é”®è¯æ–‡ä»¶ ---
              const sortedKeywordsFull = sortedKeywords.map(([k, count]) => ({ keyword: k, count }));
              await saveJsonFile(FULL_KEYWORDS_FILE, sortedKeywordsFull, fullKeywordsSha);
              
              // --- æ­¥éª¤ 5: åˆå¹¶ä¸å½’æ¡£è®ºæ–‡ ---
              const combinedPapers = [...newPapers, ...(existingData.papers || [])];
              const MAX_PAPERS_TO_KEEP = 50;
              const finalPapers = combinedPapers.slice(0, MAX_PAPERS_TO_KEEP);
              console.log(`å°†ä¿ç•™æœ€æ–°çš„ ${finalPapers.length} ç¯‡è®ºæ–‡ã€‚`);

              // --- æ­¥éª¤ 6: å‡†å¤‡å¹¶ä¿å­˜æœ€ç»ˆçš„ä¸»æ•°æ®æ–‡ä»¶ ---
              const newData = {
                papers: finalPapers,
                lastUpdate: new Date().toISOString(),
                keywords: {
                  manual: manualKeywords,
                  hot: newHotKeywords
                },
                lastAutoUpdateDate: new Date().toLocaleDateString('zh-CN', {timeZone: 'Asia/Shanghai'})
              };
              
              await saveJsonFile(DATA_FILE, newData, dataSha);
              console.log("ğŸš€ æ‰€æœ‰æ•°æ®æ›´æ–°æˆåŠŸï¼Œå·¥ä½œæµç»“æŸã€‚");
              
            } catch (error) {
              console.error("æ›´æ–°è¿‡ç¨‹ä¸­å‡ºé”™:", error);
              process.exit(1);
            }
          }
          
          main();
          EOL

      - name: Run updater
        env:
          GITHUB_TOKEN: ${{ secrets.GH_PAT }}
          PERPLEXITY_API_KEY: ${{ secrets.PERPLEXITY_API_KEY }}
        run: node update-data.js

      - name: Send email notification
        if: success()
        uses: dawidd6/action-send-mail@v5
        with:
          server_address: smtp.163.com
          server_port: 465
          secure: true
          username: 13937372851@163.com
          password: PCf7BnBBtXx9c6wk
          subject: å­¦æœ¯å‘¨æŠ¥å·²æ›´æ–° - ${{ github.repository }}
          body: |
            æ‚¨å¥½ï¼Œ
            
            æ‚¨çš„å­¦æœ¯å‘¨æŠ¥å·²æˆåŠŸæ›´æ–°ï¼æ–°çš„å­¦æœ¯è®ºæ–‡å·²ç»æ·»åŠ åˆ°æ•°æ®åº“ä¸­ã€‚
            
            æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹é“¾æ¥è®¿é—®æ‚¨çš„å­¦æœ¯å‘¨æŠ¥ï¼š
            https://shuhanliang.github.io/paper_summary/
            
            æ›´æ–°æ—¶é—´ï¼š${{ github.event.repository.updated_at }}
            ä»“åº“ï¼š${{ github.repository }}
            
            æ­¤é‚®ä»¶ç”±GitHub Actionsè‡ªåŠ¨å‘é€ï¼Œè¯·å‹¿å›å¤ã€‚
          to: blumanchu111@gmail.com, chenhui.a.ye@nokia-sbell.com
          from: å­¦æœ¯å‘¨æŠ¥è‡ªåŠ¨æ›´æ–° <13937372851@163.com>
          nodemailerlog: true
          nodemailerdebug: true
